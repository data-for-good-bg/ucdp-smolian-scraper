{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scrape webpage\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.log import configure_logging\n",
    "import logging\n",
    "from imp import reload\n",
    "reload(logging)\n",
    "import wget\n",
    "from docx import Document\n",
    "import shutil\n",
    "from io import StringIO\n",
    "## cehck on whether we have imported everything from each selishte! based on len()\n",
    "\n",
    "#OCR on scanned documents: https://www.geeksforgeeks.org/python-reading-contents-of-pdf-using-ocr-optical-character-recognition/\n",
    "    \n",
    "\n",
    "filename = f'{os.path.join(os.getcwd(),\"logging.txt\")}'\n",
    "logging.basicConfig(filename=filename,\n",
    "                    filemode='a',\n",
    "#                     encoding='utf-8',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "import re\n",
    "from datetime import datetime \n",
    "from datetime import date\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import PyPDF2\n",
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "sys.path.insert(0, \"/Users/bilyanavencheva/Desktop/Side projects/DataForGood/wood\")\n",
    "from settings import transform_clean_data, get_latest_file\n",
    "\n",
    "\n",
    "default_directory=os.path.join(os.getcwd(),'exports/files')\n",
    "protocols_directory=os.path.join(os.getcwd(),'exports/protocols')\n",
    "\n",
    "selected_files = []\n",
    "crawl_all = False\n",
    "\n",
    "\n",
    "GLOBAL_NAME = 'UIDP-SLIVEN'\n",
    "dp_name= '\"Югоизточно Държавно Предприятие\" ДП - Сливен'\n",
    "\n",
    "    \n",
    "allowed_domains = ['auction.uidp-sliven.com'] \n",
    "start_urls = ['https://auction.uidp-sliven.com/publicInfo?view=archive']\n",
    "base_url = 'https://auction.uidp-sliven.com'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "error:\n",
    "PdfReadError: Invalid Elementary Object starting with b';' @13776\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def transfer_files_to_protocol_folder():\n",
    "    #assumes all files have been UNZIPPED\n",
    "    for file in os.listdir(default_directory):\n",
    "        file_type=file.split('.')[-1]\n",
    "        file_path=os.path.join(default_directory,file)\n",
    "\n",
    "        if file_type=='pdf':\n",
    "            with open(file_path,'rb') as pdfFileObj:\n",
    "                try:\n",
    "                    pdfReader = PyPDF2.PdfFileReader(pdfFileObj,strict=False)\n",
    "                    pageObj = pdfReader.getPage(0)\n",
    "                    text = pageObj.extractText()\n",
    "                    if 'протокол'in sent_tokenize(text)[0].lower()\\\n",
    "                         or 'протокол'in sent_tokenize(text)[1].lower()\\\n",
    "                         or 'протокол'in sent_tokenize(text)[2].lower():\n",
    "                        shutil.move(file_path,os.path.join(protocols_directory,file))\n",
    "                except:\n",
    "                    print(f'{file_path.upper} cannot be opened')\n",
    "\n",
    "        if file_type=='docx':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                document = Document(f)\n",
    "                for para in document.paragraphs:\n",
    "                    if 'протокол' in para.text.lower():\n",
    "                        shutil.move(file_path,os.path.join(protocols_directory,file))\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        if file_type=='doc':\n",
    "            pass\n",
    "\n",
    "\n",
    "        if file_type not in ['pdf','doc','docx']:\n",
    "            print(file_type)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## spider Class\n",
    "class WoodSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'spider'\n",
    "    allowed_domains = allowed_domains\n",
    "    start_urls = start_urls\n",
    "    base_url = base_url\n",
    "    \n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            f'exports/archive/export_{GLOBAL_NAME}_{datetime.now()}.json': {\n",
    "                'format': 'json',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        yield scrapy.Request(self.start_urls[0], callback=self.parse_page)\n",
    "    \n",
    "    \n",
    "    def parse_page(self, response):\n",
    "        \n",
    "        #download all LINKS within the page unless they have been downloaded\n",
    "        path_links=\"//tr/td/ol/li/a[contains(concat(' ',@class, ' '), 'btn btn-link')]/@href\"\n",
    "        all_links = response.xpath(path_links).extract()\n",
    "                    \n",
    "        #download files\n",
    "        for link in all_links:\n",
    "            url=self.base_url+link\n",
    "            if url.split('/')[-1] in os.listdir(default_directory) \\\n",
    "                        or url.split('/')[-1] in os.listdir(os.path.join(default_directory,'rar')) \\\n",
    "                        or url.split('/')[-1] in os.listdir(protocols_directory):\n",
    "                pass\n",
    "            else:\n",
    "                out=default_directory if url.split('/')[-1].split('.')[-1] not in ['rar','zip','7z'] else os.path.join(default_directory,'rar')\n",
    "                wget.download(url,out=out)\n",
    "        \n",
    "        all_auctions = response.xpath('//tr/td[@class=\"btn-primary text-center\"]').extract()\n",
    "        print(len(all_auctions))\n",
    "\n",
    "        _dict = {}\n",
    "        all_content = response.xpath('//tr')\n",
    "        count = 0\n",
    "        for i,content in enumerate(all_content):\n",
    "            contents = content.xpath('.//td//text()').extract()\n",
    "            contents = [i.strip() for i in contents]\n",
    "            price_content=[i for i in contents if 'предложена цена' in i]\n",
    "\n",
    "            if len(contents) == 1 and '/ Обект №:' in contents[0]:\n",
    "                count+=1\n",
    "                _dict[count]={}\n",
    "                _dict[count]['ДП']= dp_name\n",
    "                _dict[count]['url']=np.nan\n",
    "                _dict[count]['ДГС/ДЛС'] = contents[0].split(' / ')[0]\n",
    "                _dict[count]['Обект№'] = contents[0].split(' / ')[-1].split('Обект №:')[-1]\n",
    "                _dict[count]['брой участници (споменати)']=np.nan\n",
    "                _dict[count]['участници (извлечени)']=np.nan\n",
    "                _dict[count]['брой участници (извлечени)']=np.nan\n",
    "                _dict[count]['Протокол']=np.nan\n",
    "                \n",
    "            elif len(contents)== 1 and 'е прекратена на' in contents[0]:\n",
    "                _dict[count]['други коментари'] = contents[0].split('на основание')[-1]\n",
    "\n",
    "            elif len(contents) == 2:\n",
    "                _dict[count][contents[0]] = contents[1]\n",
    "\n",
    "            elif len(contents) == 5 and 'лв.' in contents[-1] and 'ДДС.' in contents[-1]:\n",
    "                _dict[count]['Tърг№']=contents[0]\n",
    "                _dict[count]['Първа дата']=contents[1]\n",
    "                _dict[count]['Втора дата']=contents[2]\n",
    "                _dict[count]['Предмет']=contents[3]\n",
    "                _dict[count]['Начална цена']=contents[-1]\n",
    "\n",
    "            elif len(contents)>1 and 'Резултати' in contents and len(price_content)!=0:\n",
    "                while(\"\" in contents):\n",
    "                    contents.remove(\"\")\n",
    "                contestants=[i for i in contents if 'място' in i]\n",
    "                prices=[i for i in contents if 'предложена цена' in i]\n",
    "\n",
    "                _dict[count]['Първо място']=contestants[0].split('място ')[-1]\n",
    "                _dict[count]['Първа цена']=prices[0].split('размер на ')[-1]\n",
    "\n",
    "                try:\n",
    "                    _dict[count]['Второ място']=contestants[1].split('място ')[-1]\n",
    "                    _dict[count]['Втора цена']=prices[1].split('размер на ')[-1]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if len(all_auctions) != len(_dict):\n",
    "            print(f'Len auction: {len(all_auctions)} vs {len(_dict)}' )\n",
    "            \n",
    "        for k,v in _dict.items():\n",
    "            yield v\n",
    "            \n",
    "        if crawl_all == True:\n",
    "            string = \"//tr/td/ul[@class='pagination']/li/a[contains(concat(' ', i/@class, ' '), ' fa fa-chevron-right ')]/@href\"\n",
    "            next_page_partial_url = response.xpath(string).extract_first()\n",
    "\n",
    "            if next_page_partial_url:\n",
    "                next_page_url = self.base_url + next_page_partial_url\n",
    "                print(next_page_url)\n",
    "                yield scrapy.Request(next_page_url, callback=self.parse_page)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 22:57:52 [py.warnings] WARNING: /opt/anaconda3/lib/python3.8/site-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-12-16 22:57:52 [scrapy.extensions.telnet] INFO: Telnet Password: ec86b42480c8f24d\n",
      "2022-12-16 22:57:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-12-16 22:57:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n",
      "2022-12-16 22:57:53 [filelock] DEBUG: Attempting to acquire lock 140488940237584 on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 22:57:53 [filelock] INFO: Lock 140488940237584 acquired on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 22:57:53 [filelock] DEBUG: Attempting to release lock 140488940237584 on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 22:57:53 [filelock] INFO: Lock 140488940237584 released on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 22:59:01 [scrapy.extensions.logstats] INFO: Crawled 3 pages (at 3 pages/min), scraped 21 items (at 21 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=40\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=60\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=80\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=100\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 22:59:52 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 5 pages/min), scraped 121 items (at 100 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=140\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=160\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=180\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=200\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=220\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=240\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=260\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=280\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=300\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=320\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=340\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=360\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=380\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=400\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=420\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=440\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=460\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=480\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=500\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=520\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=540\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=560\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=580\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=600\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=620\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:00:52 [scrapy.extensions.logstats] INFO: Crawled 33 pages (at 25 pages/min), scraped 640 items (at 519 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=660\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=680\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=700\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=720\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=740\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=760\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=780\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=800\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=820\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=840\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=860\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=880\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=900\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=920\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=940\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=960\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=980\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1000\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1020\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1040\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1060\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1080\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:01:52 [scrapy.extensions.logstats] INFO: Crawled 56 pages (at 23 pages/min), scraped 1100 items (at 460 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1120\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1140\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1160\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1180\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1200\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1220\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1240\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1260\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1280\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1300\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1320\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1340\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1360\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1380\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1400\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1420\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1440\n",
      "20\n",
      "https://auction.uidp-sliven.com/publicInfo?view=archive&page=1460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:02:25 [scrapy.extensions.feedexport] INFO: Stored json feed (1471 items) in: exports/archive/export_UIDP-SLIVEN_2022-12-16 22:57:49.776628.json\n",
      "2022-12-16 23:02:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 28530,\n",
      " 'downloader/request_count': 75,\n",
      " 'downloader/request_method_count/GET': 75,\n",
      " 'downloader/response_bytes': 11843654,\n",
      " 'downloader/response_count': 75,\n",
      " 'downloader/response_status_count/200': 75,\n",
      " 'elapsed_time_seconds': 273.409829,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 12, 16, 21, 2, 25, 849603),\n",
      " 'item_scraped_count': 1471,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 10,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 254128128,\n",
      " 'memusage/startup': 219734016,\n",
      " 'request_depth_max': 74,\n",
      " 'response_received_count': 75,\n",
      " 'scheduler/dequeued': 75,\n",
      " 'scheduler/dequeued/memory': 75,\n",
      " 'scheduler/enqueued': 75,\n",
      " 'scheduler/enqueued/memory': 75,\n",
      " 'start_time': datetime.datetime(2022, 12, 16, 20, 57, 52, 439774)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "crawl_all=True\n",
    "process = CrawlerProcess()\n",
    "\n",
    "process.crawl(WoodSpider)\n",
    "process.start()\n",
    "\n",
    "# transfer_files_to_protocol_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file selected: \"/Users/bilyanavencheva/Desktop/Side projects/DataForGood/wood/sliven/exports/archive/export_UIDP-SLIVEN_2022-12-16 22:57:49.776628.json\"\n"
     ]
    }
   ],
   "source": [
    "# def get_latest_file(path = 'archive', filename_contains='export_'):\n",
    "#     urls = []\n",
    "#     list_of_files = [os.path.join(os.getcwd(),path,file)\n",
    "#                   for file in os.listdir(os.path.join(os.getcwd(),path)) \n",
    "#                   if filename_contains in file and '.json' in file\n",
    "#                      and os.path.getsize(os.path.join(os.getcwd(),path,file)) !=0\n",
    "#                     ]\n",
    "\n",
    "#     if len(list_of_files) != 0:\n",
    "#         latest_file = max(list_of_files, key=os.path.getctime)\n",
    "#         print(f'Latest file selected: \"{latest_file}\"')\n",
    "#         with open(latest_file ,encoding='ascii') as f:\n",
    "#             data = json.load(f)\n",
    "\n",
    "#         urls = [i['url'] for i in data]\n",
    "#     else:\n",
    "#         data = []\n",
    "#         pass\n",
    "    \n",
    "#     return urls, data\n",
    "\n",
    "latest_file, data = get_latest_file(PATH = 'exports/archive' ,type_ ='.json',aux=f'export_{GLOBAL_NAME}')\n",
    "clean_data=transform_clean_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_excel(f'{os.path.join(os.getcwd(),\"exports/excel/\")}{GLOBAL_NAME}_clean_data_{datetime.now().date()}.xlsx'\n",
    "                       ,index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
