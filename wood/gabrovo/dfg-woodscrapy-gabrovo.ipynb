{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scrape webpage\n",
    "import os\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.log import configure_logging\n",
    "import logging\n",
    "from imp import reload\n",
    "reload(logging)\n",
    "import wget\n",
    "from docx import Document\n",
    "import shutil\n",
    "from io import StringIO\n",
    "## cehck on whether we have imported everything from each selishte! based on len()\n",
    "\n",
    "#OCR on scanned documents: https://www.geeksforgeeks.org/python-reading-contents-of-pdf-using-ocr-optical-character-recognition/\n",
    "    \n",
    "\n",
    "filename = f'{os.path.join(os.getcwd(),\"logging.txt\")}'\n",
    "logging.basicConfig(filename=filename,\n",
    "                    filemode='a',\n",
    "#                     encoding='utf-8',\n",
    "                    format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    level=logging.DEBUG)\n",
    "\n",
    "import re\n",
    "from datetime import datetime \n",
    "from datetime import date\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import PyPDF2\n",
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "default_directory=os.path.join(os.getcwd(),'exports/files')\n",
    "protocols_directory=os.path.join(os.getcwd(),'exports/protocols')\n",
    "\n",
    "selected_files = []\n",
    "crawl_all = False\n",
    "\n",
    "\n",
    "GLOBAL_NAME = 'SCDP-GABROVO'\n",
    "dp_name='\"Северноцентрално държавно предприятие\" ДП - Габрово'\n",
    "\n",
    "\n",
    "allowed_domains = ['commodity-auction.scdp.bg'] \n",
    "start_urls = ['https://commodity-auction.scdp.bg/publicInfo?view=archive']\n",
    "base_url = 'https://commodity-auction.scdp.bg'\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "error:\n",
    "PdfReadError: Invalid Elementary Object starting with b';' @13776\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def transfer_files_to_protocol_folder():\n",
    "    #assumes all files have been UNZIPPED\n",
    "    for file in os.listdir(default_directory):\n",
    "        file_type=file.split('.')[-1]\n",
    "        file_path=os.path.join(default_directory,file)\n",
    "\n",
    "        if file_type=='pdf':\n",
    "            with open(file_path,'rb') as pdfFileObj:\n",
    "                try:\n",
    "                    pdfReader = PyPDF2.PdfFileReader(pdfFileObj,strict=False)\n",
    "                    pageObj = pdfReader.getPage(0)\n",
    "                    text = pageObj.extractText()\n",
    "                    if 'протокол'in sent_tokenize(text)[0].lower()\\\n",
    "                         or 'протокол'in sent_tokenize(text)[1].lower()\\\n",
    "                         or 'протокол'in sent_tokenize(text)[2].lower():\n",
    "                        shutil.move(file_path,os.path.join(protocols_directory,file))\n",
    "                except:\n",
    "                    print(f'{file_path.upper} cannot be opened')\n",
    "\n",
    "        if file_type=='docx':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                document = Document(f)\n",
    "                for para in document.paragraphs:\n",
    "                    if 'протокол' in para.text.lower():\n",
    "                        shutil.move(file_path,os.path.join(protocols_directory,file))\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "        if file_type=='doc':\n",
    "            pass\n",
    "\n",
    "\n",
    "        if file_type not in ['pdf','doc','docx']:\n",
    "            print(file_type)\n",
    "\n",
    "def transform_clean_data(data):\n",
    "    data_frame = pd.DataFrame.from_dict(data)\n",
    "    \n",
    "    assert len(data_frame) == len(data),' Raw data not same length as input file'\n",
    "    \n",
    "    data_frame.to_excel(f'{os.path.join(os.getcwd(),\"exports/excel/\")}{GLOBAL_NAME}_raw_data_{datetime.now().date()}.xlsx'\n",
    "                       ,index=False)\n",
    "    \n",
    "    clean_data = data_frame.copy()\n",
    "    \n",
    "    # adjusting columns\n",
    "    for key in ['Начална цена','Първа цена','Втора цена']:\n",
    "        for index in clean_data.index:\n",
    "            new_value = np.nan\n",
    "            try:\n",
    "                new_value = float(clean_data[key][index].split(' ')[0])# лв без ДДС / лева без ДДС\n",
    "            except:\n",
    "                new_value = clean_data[key][index]\n",
    "\n",
    "            clean_data.loc[index,key] = new_value\n",
    "                \n",
    "#     clean_data.loc[1,'месец']\n",
    "            \n",
    "    for key in ['Едра', 'Средна', 'Дребна','ОЗМ', 'Дърва за огрев', 'Общо']:\n",
    "        clean_data[key] = clean_data[key].apply(lambda x: np.nan if x.split('куб.м.')[0] == ' ' or x.split('куб.м.')[0] == ''\n",
    "                                                else float(x.split('куб.м.')[0]))\n",
    "        \n",
    "\n",
    "    # derived columns\n",
    "    clean_data['година'] = clean_data['Втора дата'].apply(lambda x: int(x.split(' ')[0].split('.')[2]))\n",
    "    clean_data['месец'] = clean_data['Втора дата'].apply(lambda x: int(x.split(' ')[0].split('.')[1]))\n",
    "    clean_data['ден'] = clean_data['Втора дата'].apply(lambda x: int(x.split(' ')[0].split('.')[0]))\n",
    "    clean_data['дата'] = clean_data.apply(lambda x: date(x['година'], x['месец'],x['ден']), axis = 1)\n",
    "    \n",
    "    if ['вид търг','предмет'].isin(clean_data.columns):\n",
    "        pass\n",
    "    else:\n",
    "        clean_data['вид търг'] = clean_data['Предмет'].apply(lambda x: 'електронен търг' \n",
    "                                                         if 'Електронен търг' in x \n",
    "                                                         else 'електронен конкурс' if 'Електронен конкурс' in x else x )\n",
    "\n",
    "        clean_data['предмет'] = clean_data['Предмет'].apply(lambda x: 'действително добити количества' \n",
    "                                                            if 'действително добити количества' in x\n",
    "                                                            else 'дървесина на прогнозни количества'  if 'дървесина на прогнозни количества' in x\n",
    "                                                            else 'дървесина на корен' if 'дървесина на корен' in x\n",
    "                                                            else np.nan\n",
    "                                                           )\n",
    "    \n",
    "    clean_data['селище'] = clean_data['ДГС/ДЛС'].apply(lambda x: x.split(' ')[2] if x.split(' ')[-1] == '' else x.split(' ')[-1])\n",
    "    clean_data['ДГС/ДЛС'] = clean_data['ДГС/ДЛС'].apply(lambda x: x.split(' ')[1])\n",
    "    \n",
    "    for key in ['ДП', 'ДГС/ДЛС','селище', 'Обект№', 'Tърг№']:\n",
    "        clean_data[key]=clean_data[key].apply(lambda x:x.strip())\n",
    "    \n",
    "    # calculated fields\n",
    "    clean_data['разлика от начална цена (лв.)'] = clean_data['Първа цена'] - clean_data['Начална цена']\n",
    "    clean_data['%от начална цена (лв.)'] = clean_data['Първа цена']/clean_data['Начална цена']\n",
    "    clean_data['%увеличение'] = clean_data['разлика от начална цена (лв.)']/clean_data['Начална цена']\n",
    "    \n",
    "    clean_data['начална цена лв./м3'] = clean_data['Начална цена'] / clean_data['Общо']\n",
    "    clean_data['крайна цена лв./м3'] = clean_data['Първа цена'] / clean_data['Общо']\n",
    "    \n",
    "    # to float\n",
    "    \n",
    "    for key in ['Начална цена','разлика от начална цена (лв.)','%от начална цена (лв.)'\n",
    "                ,'начална цена лв./м3','крайна цена лв./м3', 'Първа цена','Втора цена', 'брой участници (споменати)'\n",
    "                ,'Едра', 'Средна', 'Дребна','ОЗМ', 'Дърва за огрев', 'Общо']:\n",
    "        clean_data[key] = clean_data[key].apply(lambda x: float(x))\n",
    "    \n",
    "    # Sense checks: purva cena >= vtora cena\n",
    "    clean_data = clean_data[['дата','година', 'месец', 'ден','url', 'ДП', 'ДГС/ДЛС','селище', 'Обект№', 'Tърг№'\n",
    "                             ,'вид търг','предмет', 'Начална цена','разлика от начална цена (лв.)','%от начална цена (лв.)'\n",
    "                             ,'%увеличение','начална цена лв./м3','крайна цена лв./м3', 'Първа цена','Втора цена'\n",
    "                             ,'Първо място','Второ място', 'брой участници (споменати)','участници (извлечени)'\n",
    "                             , 'брой участници (извлечени)','Дървесен вид', 'Едра', 'Средна', 'Дребна','ОЗМ'\n",
    "                             , 'Дърва за огрев', 'Общо','други коментари'\n",
    "                            ]]\n",
    "\n",
    "    \n",
    "    #change name of columns\n",
    "    clean_data.rename(columns = {'Общо':'Обем дървесина (м3)'},inplace = True)\n",
    "    clean_data.rename(columns = {'Първа цена':'Договорена цена (лв.)'},inplace = True)\n",
    "    \n",
    "    assert len(clean_data) == len(data), 'Some values have dropped while cleaning, please check'\n",
    "    \n",
    "    clean_data.to_excel(f'{os.path.join(os.getcwd(),\"exports/excel/\")}{GLOBAL_NAME}_clean_data_{datetime.now().date()}.xlsx'\n",
    "                       ,index=False)\n",
    "    return clean_data \n",
    "            \n",
    "\n",
    "## spider Class\n",
    "class WoodSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'spider'\n",
    "    allowed_domains = allowed_domains\n",
    "    start_urls = start_urls\n",
    "    base_url = base_url\n",
    "    \n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            f'exports/archive/export_{GLOBAL_NAME}_{datetime.now()}.json': {\n",
    "                'format': 'json',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def parse(self, response):\n",
    "        yield scrapy.Request(self.start_urls[0], callback=self.parse_page)\n",
    "    \n",
    "    \n",
    "    def parse_page(self, response):\n",
    "        \n",
    "        #download all LINKS within the page unless they have been downloaded\n",
    "        path_links=\"//tr/td/ol/li/a[contains(concat(' ',@class, ' '), 'btn btn-link')]/@href\"\n",
    "        all_links = response.xpath(path_links).extract()\n",
    "                    \n",
    "        #download files\n",
    "        for link in all_links:\n",
    "            url=self.base_url+link\n",
    "            if url.split('/')[-1] in os.listdir(default_directory) \\\n",
    "                        or url.split('/')[-1] in os.listdir(os.path.join(default_directory,'rar')) \\\n",
    "                        or url.split('/')[-1] in os.listdir(protocols_directory):\n",
    "                pass\n",
    "            else:\n",
    "                out=default_directory if url.split('/')[-1].split('.')[-1] not in ['rar','zip','7z'] else os.path.join(default_directory,'rar')\n",
    "                wget.download(url,out=out)\n",
    "        \n",
    "        all_auctions = response.xpath('//tr/td[@class=\"btn-primary text-center\"]').extract()\n",
    "        print(len(all_auctions))\n",
    "\n",
    "        _dict = {}\n",
    "        all_content = response.xpath('//tr')\n",
    "        count = 0\n",
    "        for i,content in enumerate(all_content):\n",
    "            contents = content.xpath('.//td//text()').extract()\n",
    "            contents = [i.strip() for i in contents]\n",
    "            price_content=[i for i in contents if 'предложена цена' in i]\n",
    "\n",
    "            if len(contents) == 1 and '/ Обект №:' in contents[0]:\n",
    "                count+=1\n",
    "                _dict[count]={}\n",
    "                _dict[count]['ДП']= dp_name\n",
    "                _dict[count]['url']=np.nan\n",
    "                _dict[count]['ДГС/ДЛС'] = contents[0].split(' / ')[0]\n",
    "                _dict[count]['Обект№'] = contents[0].split(' / ')[-1].split('Обект №:')[-1]\n",
    "                _dict[count]['брой участници (споменати)']=np.nan\n",
    "                _dict[count]['участници (извлечени)']=np.nan\n",
    "                _dict[count]['брой участници (извлечени)']=np.nan\n",
    "                _dict[count]['Протокол']=np.nan\n",
    "                \n",
    "            elif len(contents)== 1 and 'е прекратена на' in contents[0]:\n",
    "                _dict[count]['други коментари'] = contents[0].split('на основание')[-1]\n",
    "\n",
    "            elif len(contents) == 2:\n",
    "                _dict[count][contents[0]] = contents[1]\n",
    "\n",
    "            elif len(contents) == 5 and 'лв.' in contents[-1] and 'ДДС.' in contents[-1]:\n",
    "                _dict[count]['Tърг№']=contents[0]\n",
    "                _dict[count]['Първа дата']=contents[1]\n",
    "                _dict[count]['Втора дата']=contents[2]\n",
    "                _dict[count]['Предмет']=contents[3]\n",
    "                _dict[count]['Начална цена']=contents[-1]\n",
    "\n",
    "            elif len(contents)>1 and 'Резултати' in contents and len(price_content)!=0:\n",
    "                while(\"\" in contents):\n",
    "                    contents.remove(\"\")\n",
    "                contestants=[i for i in contents if 'място' in i]\n",
    "                prices=[i for i in contents if 'предложена цена' in i]\n",
    "\n",
    "                _dict[count]['Първо място']=contestants[0].split('място ')[-1]\n",
    "                _dict[count]['Първа цена']=prices[0].split('размер на ')[-1]\n",
    "\n",
    "                try:\n",
    "                    _dict[count]['Второ място']=contestants[1].split('място ')[-1]\n",
    "                    _dict[count]['Втора цена']=prices[1].split('размер на ')[-1]\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        if len(all_auctions) != len(_dict):\n",
    "            print(f'Len auction: {len(all_auctions)} vs {len(_dict)}' )\n",
    "            \n",
    "        for k,v in _dict.items():\n",
    "            yield v\n",
    "            \n",
    "        if crawl_all == True:\n",
    "            string = \"//tr/td/ul[@class='pagination']/li/a[contains(concat(' ', i/@class, ' '), ' fa fa-chevron-right ')]/@href\"\n",
    "            next_page_partial_url = response.xpath(string).extract_first()\n",
    "\n",
    "            if next_page_partial_url:\n",
    "                next_page_url = self.base_url + next_page_partial_url\n",
    "                print(next_page_url)\n",
    "                yield scrapy.Request(next_page_url, callback=self.parse_page)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:12:57 [py.warnings] WARNING: /opt/anaconda3/lib/python3.8/site-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2022-12-16 23:12:57 [scrapy.extensions.telnet] INFO: Telnet Password: fcd27acc5422bd32\n",
      "2022-12-16 23:12:58 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-12-16 23:12:58 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6025\n",
      "2022-12-16 23:13:00 [filelock] DEBUG: Attempting to acquire lock 140694357222928 on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 23:13:00 [filelock] INFO: Lock 140694357222928 acquired on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 23:13:00 [filelock] DEBUG: Attempting to release lock 140694357222928 on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n",
      "2022-12-16 23:13:00 [filelock] INFO: Lock 140694357222928 released on /Users/bilyanavencheva/.cache/python-tldextract/3.8.8.final__anaconda3__afc312__tldextract-3.4.0/publicsuffix.org-tlds/de84b5ca2167d4c83e38fb162f2e8738.tldextract.json.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=20\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=40\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=60\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=80\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=100\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=120\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=140\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=160\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=180\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=200\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=220\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=240\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=260\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=280\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=300\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=320\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=340\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=360\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=380\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=400\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=420\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=440\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:13:58 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 24 pages/min), scraped 460 items (at 460 items/min)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=480\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=500\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=520\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=540\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=560\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=580\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=600\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=620\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=640\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=660\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=680\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=700\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=720\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=740\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=760\n",
      "20\n",
      "https://commodity-auction.scdp.bg/publicInfo?view=archive&page=780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-16 23:14:25 [scrapy.extensions.feedexport] INFO: Stored json feed (789 items) in: exports/archive/export_SCDP-GABROVO_2022-12-16 23:12:56.677128.json\n",
      "2022-12-16 23:14:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 15657,\n",
      " 'downloader/request_count': 41,\n",
      " 'downloader/request_method_count/GET': 41,\n",
      " 'downloader/response_bytes': 6603716,\n",
      " 'downloader/response_count': 41,\n",
      " 'downloader/response_status_count/200': 41,\n",
      " 'elapsed_time_seconds': 87.586313,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2022, 12, 16, 21, 14, 25, 830518),\n",
      " 'item_scraped_count': 789,\n",
      " 'log_count/DEBUG': 2,\n",
      " 'log_count/INFO': 7,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 236732416,\n",
      " 'memusage/startup': 215470080,\n",
      " 'request_depth_max': 40,\n",
      " 'response_received_count': 41,\n",
      " 'scheduler/dequeued': 41,\n",
      " 'scheduler/dequeued/memory': 41,\n",
      " 'scheduler/enqueued': 41,\n",
      " 'scheduler/enqueued/memory': 41,\n",
      " 'start_time': datetime.datetime(2022, 12, 16, 21, 12, 58, 244205)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "crawl_all=True\n",
    "process = CrawlerProcess()\n",
    "\n",
    "process.crawl(WoodSpider)\n",
    "process.start()\n",
    "\n",
    "# transfer_files_to_protocol_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest file selected: \"/Users/bilyanavencheva/Desktop/Side projects/DataForGood/wood/gabrovo/exports/archive/export_SCDP-GABROVO_2022-12-16 23:12:56.677128.json\"\n"
     ]
    }
   ],
   "source": [
    "def get_latest_file(path = 'exports/archive', filename_contains='export_'):\n",
    "    urls = []\n",
    "    list_of_files = [os.path.join(os.getcwd(),path,file)\n",
    "                  for file in os.listdir(os.path.join(os.getcwd(),path)) \n",
    "                  if filename_contains in file and '.json' in file\n",
    "                     and os.path.getsize(os.path.join(os.getcwd(),path,file)) !=0\n",
    "                    ]\n",
    "\n",
    "    if len(list_of_files) != 0:\n",
    "        latest_file = max(list_of_files, key=os.path.getctime)\n",
    "        print(f'Latest file selected: \"{latest_file}\"')\n",
    "        with open(latest_file ,encoding='ascii') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        urls = [i['url'] for i in data]\n",
    "    else:\n",
    "        data = []\n",
    "        pass\n",
    "    \n",
    "    return urls, data\n",
    "\n",
    "urls, data = get_latest_file(path = 'exports/archive', filename_contains='export_')\n",
    "\n",
    "clean_data=transform_clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
